{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Models Part 1 - For Df 1 and Df 2\n",
        "\n"
      ],
      "metadata": {
        "id": "d_F6rp7gRxiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare our datasets for analysis, we took the following steps:\n",
        "\n",
        "\n",
        "*  **Loading Datasets:** We loaded Dataset 1 and Dataset 2 to clean inconsistencies and ensure the data was ready for modeling. This step involved reading the data into a manageable format for preprocessing.  \n",
        "*   **Cleaning Dataset 1:** In Dataset 1, we renamed columns for clarity, encoded binary responses such as \"Yes\" and \"No\" into numeric values for better compatibility with machine learning models, and scaled numerical features like age and academic pressure to ensure uniformity across features.\n",
        "*  **Cleaning Dataset 2:** For Dataset 2, we handled complex entries such as CGPA ranges and sleep durations by converting them into numeric values suitable for modeling. Additionally, we dropped rows with missing values to ensure a clean and reliable dataset for regression analysis.\n"
      ],
      "metadata": {
        "id": "wVwdQW7qbZ_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS6hYxrEQHAC"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load datasets\n",
        "dataset1_url = \"https://raw.githubusercontent.com/hbedros/data622-assignment4/main/data/dataset1.csv\"\n",
        "dataset2_url = \"https://raw.githubusercontent.com/hbedros/data622-assignment4/main/data/dataset2.csv\"\n",
        "df1 = pd.read_csv(dataset1_url)\n",
        "df2 = pd.read_csv(dataset2_url)\n",
        "\n",
        "# Clean Dataset 1\n",
        "df1.columns = df1.columns.str.strip()\n",
        "df1 = df1.rename(columns={'Have you ever had suicidal thoughts ?': 'suicidal_thoughts'})\n",
        "df1['suicidal_thoughts'] = df1['suicidal_thoughts'].map({'Yes': 1, 'No': 0})\n",
        "df1['Depression'] = df1['Depression'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Scale numerical columns\n",
        "numeric_cols_df1 = ['Age', 'Academic Pressure', 'Study Satisfaction', 'Study Hours', 'Financial Stress']\n",
        "scaler = MinMaxScaler()\n",
        "df1[numeric_cols_df1] = scaler.fit_transform(df1[numeric_cols_df1])\n",
        "\n",
        "# Clean Dataset 2\n",
        "df2.columns = df2.columns.str.strip()\n",
        "\n",
        "# Parse CGPA ranges\n",
        "def parse_range(val):\n",
        "    if \"-\" in val:\n",
        "        lower, upper = map(float, val.split(\"-\"))\n",
        "        return (lower + upper) / 2\n",
        "    else:\n",
        "        return float(val)\n",
        "\n",
        "df2['cgpa'] = df2['cgpa'].apply(parse_range)\n",
        "\n",
        "# Clean 'average_sleep' column\n",
        "def parse_sleep(val):\n",
        "    if isinstance(val, str):\n",
        "        val = val.replace(\" hrs\", \"\").strip()\n",
        "        if \"-\" in val:\n",
        "            lower, upper = map(float, val.split(\"-\"))\n",
        "            return (lower + upper) / 2\n",
        "        elif val.isdigit():\n",
        "            return float(val)\n",
        "    return np.nan\n",
        "\n",
        "df2['average_sleep'] = df2['average_sleep'].apply(parse_sleep)\n",
        "\n",
        "# Feature scaling for numerical columns\n",
        "numeric_cols_df2 = ['cgpa', 'study_satisfaction', 'average_sleep']\n",
        "df2[numeric_cols_df2] = scaler.fit_transform(df2[numeric_cols_df2])\n",
        "\n",
        "# Drop rows with missing values\n",
        "df2 = df2.dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis on Dataset 1 (Classification)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xxl2rdz4YlfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepared Dataset 1 for modeling by scaling numerical features and encoding categorical variables. The data was split into training and testing sets to predict the binary outcome (Depression). Classification models such as Decision Tree, Random Forest, and Logistic Regression were chosen to align with the dataset's classification nature. Model performance was evaluated based on accuracy."
      ],
      "metadata": {
        "id": "Yw0JzOYJZuKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why These Models Were Used for Dataset 1:**\n",
        "\n",
        "\n",
        "\n",
        "*   **Decision Tree:** Selected as a simple starting model for classification. It provides clear decision-making rules, ideal for exploring how features like age, academic pressure, and sleep duration relate to Depression. This aligns with Dataset 1â€™s classification objective.\n",
        "*   **Random Forest:** Built on the Decision Tree by aggregating results from multiple trees. This ensemble approach reduces overfitting and enhances prediction accuracy, making it effective for capturing complex feature interactions.\n",
        "\n",
        "*  **Logistic Regression:** Chosen as a linear baseline model for comparison. Its ability to handle binary classification tasks efficiently is suitable for predicting Depression, particularly when relationships between features and the target are linear."
      ],
      "metadata": {
        "id": "0dxrXk-AjLJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Dataset 1 - Preprocessing\n",
        "X1 = df1.drop(columns=['Depression'])\n",
        "y1 = df1['Depression']\n",
        "\n",
        "# Apply OneHotEncoding for categorical variables and scaling for numeric variables\n",
        "categorical_cols = X1.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X1.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X1_preprocessed = preprocessor.fit_transform(X1)\n",
        "\n",
        "# Train-test split\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1_preprocessed, y1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X1_train, y1_train)\n",
        "y1_pred_dt = dt_clf.predict(X1_test)\n",
        "dt_accuracy = accuracy_score(y1_test, y1_pred_dt)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X1_train, y1_train)\n",
        "y1_pred_rf = rf_clf.predict(X1_test)\n",
        "rf_accuracy = accuracy_score(y1_test, y1_pred_rf)\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X1_train, y1_train)\n",
        "y1_pred_lr = log_reg.predict(X1_test)\n",
        "lr_accuracy = accuracy_score(y1_test, y1_pred_lr)\n",
        "\n",
        "# Tabulate Results for Dataset 1\n",
        "results_df1 = pd.DataFrame({\n",
        "    \"Data Table\": [\"DF 1\", \"DF 1\", \"DF 1\"],\n",
        "    \"Model\": [\"Decision Tree\", \"Random Forest\", \"Logistic Regression\"],\n",
        "    \"Accuracy\": [dt_accuracy, rf_accuracy, lr_accuracy],\n",
        "    \"MAE\": [None, None, None],\n",
        "    \"RMSE\": [None, None, None]\n",
        "})\n",
        "\n",
        "print(results_df1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G83GjPZyYqVZ",
        "outputId": "6fec249a-0d59-4904-9e20-f7d9857527ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Data Table                Model  Accuracy   MAE  RMSE\n",
            "0       DF 1        Decision Tree  0.881188  None  None\n",
            "1       DF 1        Random Forest  0.950495  None  None\n",
            "2       DF 1  Logistic Regression  0.960396  None  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results Explained for Dataset 1:**\n",
        "\n",
        "\n",
        "\n",
        "*   **Decision Tree:** Achieved an accuracy of **88.12%**, making it a solid starting model. While it provides clear and interpretable decision paths, its lower accuracy suggests it may not capture complex patterns in the data as effectively as the other models.\n",
        "*   **Random Forest:** Performed significantly better with an accuracy of **95.05%**. By averaging the outputs of multiple decision trees, it reduces overfitting and better handles the complexity of the dataset, leading to improved predictions.\n",
        "*   **Logistic Regression:** Delivered the highest accuracy of **96.04%**, indicating that the relationships in the dataset are well-suited to a linear model. This suggests features like age, academic pressure, and financial stress have a straightforward influence on predicting depression in this context.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oIQf7mK0bNgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis on Dataset 2 (Regression Analysis)"
      ],
      "metadata": {
        "id": "qDonpyPXcEfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepared Dataset 2 to predict CGPA (Cumulative Grade Point Average), a measure of a studentâ€™s academic performance, using factors like study satisfaction, workload, and sleep patterns. Numerical features were scaled, categorical variables encoded, and regression models such as Linear Regression, Decision Tree Regressor, Random Forest, and Gradient Boosting were applied. The data was split into training and testing sets, and model performance was evaluated using RMSE and MAE to assess predictive accuracy."
      ],
      "metadata": {
        "id": "G_goCZqcisSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Linear Regression:** Selected as a baseline regression model to predict CGPA. Its simplicity and suitability for linear relationships between features and the target make it a logical starting point.\n",
        "*   **Decision Tree:** Chosen for its ability to capture non-linear relationships, this model helps understand the impact of factors like academic workload and sleep duration on CGPA. This aligns with Dataset 2's nature as a regression problem.\n",
        "*   **Random Forest:** Built on the Decision Tree to enhance prediction accuracy by averaging the results of multiple trees. This reduces overfitting and captures complex feature interactions effectively.\n",
        "*   **Gradient Boosting:** Selected to refine predictions iteratively, improving accuracy over other regression models. Its strength lies in handling non-linear relationships and feature interactions, making it well-suited for this dataset."
      ],
      "metadata": {
        "id": "V2vrBO9zjQFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Define target variable and features\n",
        "X2 = df2.drop(columns=[\"cgpa\"])\n",
        "y2 = df2[\"cgpa\"]\n",
        "\n",
        "# Preprocessing\n",
        "categorical_cols = X2.select_dtypes(include=[\"object\"]).columns\n",
        "numerical_cols = X2.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numerical_cols),\n",
        "        (\"cat\", OneHotEncoder(), categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X2_preprocessed = preprocessor.fit_transform(X2)\n",
        "\n",
        "# Train-test split\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_preprocessed, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X2_train, y2_train)\n",
        "y2_pred_lr = lr.predict(X2_test)\n",
        "lr_rmse = np.sqrt(mean_squared_error(y2_test, y2_pred_lr))\n",
        "lr_mae = mean_absolute_error(y2_test, y2_pred_lr)\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X2_train, y2_train)\n",
        "y2_pred_dt = dt.predict(X2_test)\n",
        "dt_rmse = np.sqrt(mean_squared_error(y2_test, y2_pred_dt))\n",
        "dt_mae = mean_absolute_error(y2_test, y2_pred_dt)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X2_train, y2_train)\n",
        "y2_pred_rf = rf.predict(X2_test)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y2_test, y2_pred_rf))\n",
        "rf_mae = mean_absolute_error(y2_test, y2_pred_rf)\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb.fit(X2_train, y2_train)\n",
        "y2_pred_gb = gb.predict(X2_test)\n",
        "gb_rmse = np.sqrt(mean_squared_error(y2_test, y2_pred_gb))\n",
        "gb_mae = mean_absolute_error(y2_test, y2_pred_gb)\n",
        "\n",
        "# Tabulate Results for Dataset 2\n",
        "results_df2 = pd.DataFrame({\n",
        "    \"Data Table\": [\"DF 2\", \"DF 2\", \"DF 2\", \"DF 2\"],\n",
        "    \"Model\": [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"],\n",
        "    \"Accuracy\": [None, None, None, None],\n",
        "    \"MAE\": [lr_mae, dt_mae, rf_mae, gb_mae],\n",
        "    \"RMSE\": [lr_rmse, dt_rmse, rf_rmse, gb_rmse]\n",
        "})\n",
        "\n",
        "print(results_df2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnYcxCE8cQZi",
        "outputId": "8d5bf1c1-8151-4bf1-d27e-ac31938a2a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Data Table              Model Accuracy       MAE      RMSE\n",
            "0       DF 2  Linear Regression     None  0.345027  0.447927\n",
            "1       DF 2      Decision Tree     None  0.214815  0.352066\n",
            "2       DF 2      Random Forest     None  0.158963  0.230364\n",
            "3       DF 2  Gradient Boosting     None  0.186680  0.255632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results Explained for Dataset 2:**\n",
        "\n",
        "*  **Linear Regression:** Achieved an RMSE of 0.4479 and MAE of 0.3450. While it identifies some linear trends, it struggles to capture the complexity of how factors like study satisfaction and sleep patterns influence CGPA.\n",
        "\n",
        "*   **Decision Tree:** Improved with an RMSE of 0.3521 and MAE of 0.2148. Its ability to handle non-linear relationships makes it better at identifying the effects of features like academic workload on CGPA.\n",
        "*   **Random Forest:** Delivered the best performance with an RMSE of 0.2304 and MAE of 0.1590. By averaging multiple trees, it effectively captures complex interactions and provides the most accurate predictions.\n",
        "*  **Gradient Boosting:** Achieved an RMSE of 0.2556 and MAE of 0.1867. Its iterative improvements make it a strong model, though slightly less accurate than Random Forest for this dataset.\n",
        "\n",
        "\n",
        "**Key Takeaway:**\n",
        "\n",
        "Random Forest excelled in predicting CGPA, showcasing its ability to model complex relationships between academic and personal factors."
      ],
      "metadata": {
        "id": "oUP2-gVbjHSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "OAUsOOjQoIgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The analysis of both datasets highlights the importance of selecting models that align with the data and objectives.  \n",
        "\n",
        "- For **Dataset 1**, focused on predicting Depression:  \n",
        "  - **Logistic Regression** achieved the highest accuracy, effectively handling the binary classification task.  \n",
        "  - This suggests that the relationships between features like age, financial stress, and academic pressure are well-suited to a linear model.  \n",
        "\n",
        "- For **Dataset 2**, which aimed to predict CGPA:  \n",
        "  - **Random Forest** delivered the best performance, excelling at capturing complex interactions between features such as academic workload, sleep duration, and social relationships.  \n",
        "  - Its ensemble approach minimized errors and provided the most accurate predictions.  \n",
        "\n",
        "- Overall:  \n",
        "  - The methodologiesâ€”classification for Dataset 1 and regression for Dataset 2â€”were well-matched to the nature of each dataset.  \n",
        "  - **Random Forest** stood out for its versatility and accuracy in handling complex datasets.  "
      ],
      "metadata": {
        "id": "CZ3oebAZoLZS"
      }
    }
  ]
}
